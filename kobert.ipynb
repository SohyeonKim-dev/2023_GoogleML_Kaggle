{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-05T11:40:49.395189Z","iopub.execute_input":"2023-11-05T11:40:49.395532Z","iopub.status.idle":"2023-11-05T11:40:49.408130Z","shell.execute_reply.started":"2023-11-05T11:40:49.395496Z","shell.execute_reply":"2023-11-05T11:40:49.407129Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"/kaggle/input/korean-hate-speech-detection/test.news_title.txt\n/kaggle/input/korean-hate-speech-detection/test.hate.no_label.csv\n/kaggle/input/korean-hate-speech-detection/dev.hate.csv\n/kaggle/input/korean-hate-speech-detection/dev.news_title.txt\n/kaggle/input/korean-hate-speech-detection/unlabeled_comments.txt\n/kaggle/input/korean-hate-speech-detection/unlabeled_comments.news_title.txt\n/kaggle/input/korean-hate-speech-detection/train.hate.csv\n/kaggle/input/korean-hate-speech-detection/train.news_title.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q -U transformers ","metadata":{"execution":{"iopub.status.busy":"2023-11-05T10:38:39.760322Z","iopub.execute_input":"2023-11-05T10:38:39.760558Z","iopub.status.idle":"2023-11-05T10:39:01.853432Z","shell.execute_reply.started":"2023-11-05T10:38:39.760528Z","shell.execute_reply":"2023-11-05T10:39:01.852364Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 1.14.0 requires huggingface-hub<0.1.0,>=0.0.19, but you have huggingface-hub 0.16.4 which is incompatible.\nallennlp 2.7.0 requires transformers<4.10,>=4.1, but you have transformers 4.30.2 which is incompatible.\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import pipeline\n\npipe = pipeline('text-classification', model='beomi/beep-KcELECTRA-base-hate', device=0)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T10:39:01.855607Z","iopub.execute_input":"2023-11-05T10:39:01.855856Z","iopub.status.idle":"2023-11-05T10:39:31.147821Z","shell.execute_reply.started":"2023-11-05T10:39:01.855826Z","shell.execute_reply":"2023-11-05T10:39:31.146998Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aace9e7a341d43e685ac6f2893d94bce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/498M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74b50181d4b0428390509afb7c13da83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/544 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb51a983b0f445a7aedc193645d4413a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/396k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77a41946ca50487ca95835f614c2a4f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/788k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acf5f4daa8a4438ba7cc518d83b5174f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b28bbf4be0974c70930a0c61675ece2b"}},"metadata":{}},{"name":"stderr","text":"Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\npip install xformers.\n","output_type":"stream"}]},{"cell_type":"code","source":"pipe(\"안녕하세요\")","metadata":{"execution":{"iopub.status.busy":"2023-11-05T10:39:31.149878Z","iopub.execute_input":"2023-11-05T10:39:31.150282Z","iopub.status.idle":"2023-11-05T10:39:32.371974Z","shell.execute_reply.started":"2023-11-05T10:39:31.150237Z","shell.execute_reply":"2023-11-05T10:39:32.371214Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"[{'label': 'none', 'score': 0.9638350009918213}]"},"metadata":{}}]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/korean-hate-speech-detection/test.hate.no_label.csv')","metadata":{"execution":{"iopub.status.busy":"2023-11-05T10:39:34.676411Z","iopub.execute_input":"2023-11-05T10:39:34.676716Z","iopub.status.idle":"2023-11-05T10:39:34.696194Z","shell.execute_reply.started":"2023-11-05T10:39:34.676680Z","shell.execute_reply":"2023-11-05T10:39:34.695483Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2023-11-05T10:39:34.974854Z","iopub.execute_input":"2023-11-05T10:39:34.975580Z","iopub.status.idle":"2023-11-05T10:39:34.992777Z","shell.execute_reply.started":"2023-11-05T10:39:34.975545Z","shell.execute_reply":"2023-11-05T10:39:34.991973Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                              comments\n0         ㅋㅋㅋㅋ 그래도 조아해주는 팬들 많아서 좋겠다 ㅠㅠ 니들은 온유가 안만져줌 ㅠㅠ\n1                                        둘다 넘 좋다~행복하세요\n2                 근데 만원이하는 현금결제만 하라고 써놓은집 우리나라에 엄청 많은데\n3                원곡생각하나도 안나고 러블리즈 신곡나온줄!!! 너무 예쁘게 잘봤어요\n4                                   장현승 얘도 참 이젠 짠하다...\n..                                                 ...\n969                     대박 게스트... 꼭 봐야징~ 컨셉이 바뀌니깐 재미지넹\n970  성형으로 다 뜯어고쳐놓고 예쁜척. 성형 전 니 얼굴 다 알고있다. 순자처럼 된장냄새...\n971  분위기는 비슷하다만 전혀다른 전개던데 무슨ㅋㅋㄱ 우리나라사람들은 분위기만 비슷하면 ...\n972                               입에 손가릭이 10개 있으니 징그럽다\n973                              난 조보아 이뻐서 보는데 백종원 관심무\n\n[974 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comments</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ㅋㅋㅋㅋ 그래도 조아해주는 팬들 많아서 좋겠다 ㅠㅠ 니들은 온유가 안만져줌 ㅠㅠ</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>둘다 넘 좋다~행복하세요</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>근데 만원이하는 현금결제만 하라고 써놓은집 우리나라에 엄청 많은데</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>원곡생각하나도 안나고 러블리즈 신곡나온줄!!! 너무 예쁘게 잘봤어요</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>장현승 얘도 참 이젠 짠하다...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>969</th>\n      <td>대박 게스트... 꼭 봐야징~ 컨셉이 바뀌니깐 재미지넹</td>\n    </tr>\n    <tr>\n      <th>970</th>\n      <td>성형으로 다 뜯어고쳐놓고 예쁜척. 성형 전 니 얼굴 다 알고있다. 순자처럼 된장냄새...</td>\n    </tr>\n    <tr>\n      <th>971</th>\n      <td>분위기는 비슷하다만 전혀다른 전개던데 무슨ㅋㅋㄱ 우리나라사람들은 분위기만 비슷하면 ...</td>\n    </tr>\n    <tr>\n      <th>972</th>\n      <td>입에 손가릭이 10개 있으니 징그럽다</td>\n    </tr>\n    <tr>\n      <th>973</th>\n      <td>난 조보아 이뻐서 보는데 백종원 관심무</td>\n    </tr>\n  </tbody>\n</table>\n<p>974 rows × 1 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['label'] = df['comments'].map(lambda x: pipe(x)[0]['label'])","metadata":{"execution":{"iopub.status.busy":"2023-11-05T10:39:35.215288Z","iopub.execute_input":"2023-11-05T10:39:35.216113Z","iopub.status.idle":"2023-11-05T10:39:43.788430Z","shell.execute_reply.started":"2023-11-05T10:39:35.216059Z","shell.execute_reply":"2023-11-05T10:39:43.787672Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/pipelines/base.py:1084: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n  UserWarning,\n","output_type":"stream"}]},{"cell_type":"code","source":"df.label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-11-05T10:40:04.481684Z","iopub.execute_input":"2023-11-05T10:40:04.482407Z","iopub.status.idle":"2023-11-05T10:40:04.492295Z","shell.execute_reply.started":"2023-11-05T10:40:04.482369Z","shell.execute_reply":"2023-11-05T10:40:04.491356Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"none         405\noffensive    334\nhate         235\nName: label, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"LABEL_DIC = {\n    'none': 0,\n    'offensive': 1,\n    'hate': 2,\n}","metadata":{"execution":{"iopub.status.busy":"2023-11-05T10:40:04.761605Z","iopub.execute_input":"2023-11-05T10:40:04.761937Z","iopub.status.idle":"2023-11-05T10:40:04.766588Z","shell.execute_reply.started":"2023-11-05T10:40:04.761899Z","shell.execute_reply":"2023-11-05T10:40:04.765794Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df['label'] = df['label'].map(lambda x: LABEL_DIC[x])","metadata":{"trusted":true},"execution_count":37,"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_20/864088669.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    lambda\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (864088669.py, line 1)","output_type":"error"}]},{"cell_type":"code","source":"df.to_csv('./submission.csv', index=None)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T10:40:05.983125Z","iopub.execute_input":"2023-11-05T10:40:05.983442Z","iopub.status.idle":"2023-11-05T10:40:05.996023Z","shell.execute_reply.started":"2023-11-05T10:40:05.983402Z","shell.execute_reply":"2023-11-05T10:40:05.995107Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"!head -n 10 submission.csv","metadata":{"execution":{"iopub.status.busy":"2023-11-05T10:40:06.226558Z","iopub.execute_input":"2023-11-05T10:40:06.227323Z","iopub.status.idle":"2023-11-05T10:40:07.315828Z","shell.execute_reply.started":"2023-11-05T10:40:06.227282Z","shell.execute_reply":"2023-11-05T10:40:07.314959Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\ncomments,label\nㅋㅋㅋㅋ 그래도 조아해주는 팬들 많아서 좋겠다 ㅠㅠ 니들은 온유가 안만져줌 ㅠㅠ,1\n둘다 넘 좋다~행복하세요,0\n근데 만원이하는 현금결제만 하라고 써놓은집 우리나라에 엄청 많은데,0\n원곡생각하나도 안나고 러블리즈 신곡나온줄!!! 너무 예쁘게 잘봤어요,0\n장현승 얘도 참 이젠 짠하다...,0\n신선하게 웃긴다ㅋㅋㅋ역시 동엽신~~!! 장소연님은 진짜 조선족인가 착각할정도로 말투가 리얼하네요,1\n누군데 얘네?,1\n\"하자 인생들 모아다가 방송에 내보내고, 덜 하자가 교정해서 장사 풀리게 해주는 감동 스토리 백하자의 골목식당. 호텔 말고 그냥 하자 거리를 하나 열어서 거기다 하자 인생들 교화소를 만들지... 왜 저러고 살까...\",2\n진짜 라디오 스타 노래한거 보세요 홍진영은비비지도 못함,0\n","output_type":"stream"}]},{"cell_type":"code","source":"print('finish')","metadata":{"execution":{"iopub.status.busy":"2023-11-05T10:40:07.345469Z","iopub.execute_input":"2023-11-05T10:40:07.345772Z","iopub.status.idle":"2023-11-05T10:40:07.350842Z","shell.execute_reply.started":"2023-11-05T10:40:07.345738Z","shell.execute_reply":"2023-11-05T10:40:07.349934Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"finish\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch\n\n# Hugging Face의 사전 훈련된 KoBERT 모델 및 토크나이저를 사용\nmodel_name = \"monologg/kobert\"\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# 문장을 입력으로 받아 토큰화하고 모델로 분류\ndef classify_sentence(sentence):\n    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n    outputs = model(**inputs)\n    logits = outputs.logits\n    probabilities = torch.softmax(logits, dim=1)[0].tolist()\n    \n    # offensive 여부 판단\n    offensive_threshold = 0.5\n    if probabilities[1] > offensive_threshold:\n        return \"offensive\"\n    elif probabilities[0] > offensive_threshold:\n        return \"hate\"\n    else:\n        return \"None\"\n\n# 사용 예시\nsentence = \"이 문장은 정말 나쁘다!\"\nresult = classify_sentence(sentence)\nprint(f\"입력 문장: {sentence}\")\nprint(f\"분류 결과: {result}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-05T11:41:04.738811Z","iopub.execute_input":"2023-11-05T11:41:04.739135Z","iopub.status.idle":"2023-11-05T11:41:05.873723Z","shell.execute_reply.started":"2023-11-05T11:41:04.739096Z","shell.execute_reply":"2023-11-05T11:41:05.872798Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"입력 문장: 이 문장은 정말 나쁘다!\n분류 결과: hate\n","output_type":"stream"}]},{"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/korean-hate-speech-detection/test.hate.no_label.csv')","metadata":{"execution":{"iopub.status.busy":"2023-11-05T11:41:10.793548Z","iopub.execute_input":"2023-11-05T11:41:10.794625Z","iopub.status.idle":"2023-11-05T11:41:10.805697Z","shell.execute_reply.started":"2023-11-05T11:41:10.794575Z","shell.execute_reply":"2023-11-05T11:41:10.805018Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"numOfData = len(df_test[\"comments\"])\n# numOfData = 974\n\nLABEL_DIC = {\n    'None': 0,\n    'offensive': 1,\n    'hate': 2,\n}\n\nfor i in range(numOfData):\n    sentence = df_test[\"comments\"][i]\n    ans = classify_sentence(sentence)\n    label = LABEL_DIC.get(ans, 0)\n    label = int(label)\n    df_test.at[i, 'label'] = label\n\n# 데이터프레임을 CSV 파일로 저장","metadata":{"execution":{"iopub.status.busy":"2023-11-05T11:41:11.196031Z","iopub.execute_input":"2023-11-05T11:41:11.196913Z","iopub.status.idle":"2023-11-05T11:42:16.957997Z","shell.execute_reply.started":"2023-11-05T11:41:11.196868Z","shell.execute_reply":"2023-11-05T11:42:16.957167Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"print(label)\ndf_test['label'][0]\n\n# float_format='%.0f' 로 해결 ","metadata":{"execution":{"iopub.status.busy":"2023-11-05T11:42:38.995161Z","iopub.execute_input":"2023-11-05T11:42:38.995721Z","iopub.status.idle":"2023-11-05T11:42:39.002304Z","shell.execute_reply.started":"2023-11-05T11:42:38.995680Z","shell.execute_reply":"2023-11-05T11:42:39.001543Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"1\n","output_type":"stream"},{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"2.0"},"metadata":{}}]},{"cell_type":"code","source":"df_test.to_csv('./submission_test_int.csv', index=None, float_format='%.0f')\n\n# 성능: 0.25284","metadata":{"execution":{"iopub.status.busy":"2023-11-05T11:43:51.297876Z","iopub.execute_input":"2023-11-05T11:43:51.298727Z","iopub.status.idle":"2023-11-05T11:43:51.310378Z","shell.execute_reply.started":"2023-11-05T11:43:51.298682Z","shell.execute_reply":"2023-11-05T11:43:51.309539Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"# # https://github.com/SKTBrain/KoBERT \n\n# first = df[\"comments\"][0]\n\n# classify_sentence(first)\n\n# first_ans = df[\"label\"][0]\n# print(first_ans)\n\n\n# # fail or success\n\n# numOfData = len(df[\"comments\"])\n# # numOfData = 974\n\n# success = 0\n# fail = 0\n\n# LABEL_DIC = {\n#     'None': 0,\n#     'offensive': 1,\n#     'hate': 2,\n# }\n\n# for i in range (numOfData):\n#     sentence = df[\"comments\"][i]\n#     ans = classify_sentence(sentence)\n#     ans = [LABEL_DIC.get(x, 0) for x in ans]\n    \n#     if all(ans == df[\"label\"][i]):\n#         success = success + 1\n#     else:\n#         fail = fail + 1\n\n# print(success)\n# print(fail)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T11:12:46.094241Z","iopub.execute_input":"2023-11-05T11:12:46.094546Z","iopub.status.idle":"2023-11-05T11:13:50.092131Z","shell.execute_reply.started":"2023-11-05T11:12:46.094512Z","shell.execute_reply":"2023-11-05T11:13:50.091283Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"405\n569\n","output_type":"stream"}]},{"cell_type":"code","source":"# from transformers import AutoModelForSequenceClassification, AutoTokenizer, T5ForConditionalGeneration, T5Tokenizer\n\n# # Hugging Face의 사전 훈련된 T5 모델 및 토크나이저를 사용\n# model_name = \"t5-small\"\n# model = T5ForConditionalGeneration.from_pretrained(model_name)\n# tokenizer = T5Tokenizer.from_pretrained(model_name)\n\n# # 문장을 입력으로 받아 토큰화하고 모델로 분류\n# def classify_sentence(sentence):\n#     # T5 모델은 입력 문장을 \"classify: \"라는 접두어로 붙여줘야 합니다.\n#     input_text = \"classify: \" + sentence\n#     inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n#     outputs = model.generate(**inputs)\n#     decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n#     # 분류 결과 확인\n#     labels = [\"hate\", \"offensive\", \"None\"]\n#     return decoded_output\n\n# # 예제 문장을 분류해보기\n# example_sentence = \"이 문장은 모욕적이에요!\"\n# classification = classify_sentence(example_sentence)\n# print(f\"분류 결과: {classification}\")\n\n# # 결과가 이상하게 나옴\n# # 분류 결과: Klassify:  !","metadata":{"execution":{"iopub.status.busy":"2023-11-05T11:17:19.595344Z","iopub.execute_input":"2023-11-05T11:17:19.596089Z","iopub.status.idle":"2023-11-05T11:17:21.007929Z","shell.execute_reply.started":"2023-11-05T11:17:19.596033Z","shell.execute_reply":"2023-11-05T11:17:21.007125Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"분류 결과: Klassify:  !\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}