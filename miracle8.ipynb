{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZO3MDrbfpfxV"},"source":["huggingface 패키지를 Colab에 설치합니다"]},{"cell_type":"markdown","source":["출처: https://github.com/hayul7805/Korean-Counter-speech/blob/main/HSD-model-evaluation.ipynb"],"metadata":{"id":"EP9jK05TYp-f"}},{"cell_type":"code","metadata":{"id":"UoENjVEcchSP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700797142415,"user_tz":-540,"elapsed":42354,"user":{"displayName":"김소현","userId":"12131249549978937836"}},"outputId":"12950a4d-5eef-4351-f61a-c1534f4db28c"},"source":["!pip install transformers\n","!pip install sentencepiece\n","!pip install tensorflow_addons"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n","Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.10/dist-packages (0.22.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.2)\n","Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (2.13.3)\n"]}]},{"cell_type":"markdown","metadata":{"id":"Pdvo-EpaqFev"},"source":["텐서플로우 2와 필요한 모듈들을 임포트합니다.  \n","최근에 텐서플로우 기본 버전은 2로 바뀌었습니다."]},{"cell_type":"code","source":["!pip install tensorflow==2.14.0\n","\n","!pip install --force-reinstall transformers==4.15.0"],"metadata":{"id":"RXUl29flJBXs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700797193657,"user_tz":-540,"elapsed":51253,"user":{"displayName":"김소현","userId":"12131249549978937836"}},"outputId":"dcaf5b5e-d7d1-428e-a3d4-b0f430538189"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow==2.14.0 in /usr/local/lib/python3.10/dist-packages (2.14.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (23.5.26)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (16.0.6)\n","Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (0.2.0)\n","Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (23.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (4.5.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (0.34.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (1.59.2)\n","Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (2.14.1)\n","Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (2.14.0)\n","Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (2.14.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.14.0) (0.41.3)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.5.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.2.2)\n","Collecting transformers==4.15.0\n","  Using cached transformers-4.15.0-py3-none-any.whl (3.4 MB)\n","Collecting filelock (from transformers==4.15.0)\n","  Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n","Collecting huggingface-hub<1.0,>=0.1.0 (from transformers==4.15.0)\n","  Using cached huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n","Collecting numpy>=1.17 (from transformers==4.15.0)\n","  Using cached numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n","Collecting packaging>=20.0 (from transformers==4.15.0)\n","  Using cached packaging-23.2-py3-none-any.whl (53 kB)\n","Collecting pyyaml>=5.1 (from transformers==4.15.0)\n","  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n","Collecting regex!=2019.12.17 (from transformers==4.15.0)\n","  Using cached regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n","Collecting requests (from transformers==4.15.0)\n","  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n","Collecting sacremoses (from transformers==4.15.0)\n","  Using cached sacremoses-0.1.1-py3-none-any.whl (897 kB)\n","Collecting tokenizers<0.11,>=0.10.1 (from transformers==4.15.0)\n","  Using cached tokenizers-0.10.3.tar.gz (212 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting tqdm>=4.27 (from transformers==4.15.0)\n","  Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n","Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.1.0->transformers==4.15.0)\n","  Using cached fsspec-2023.10.0-py3-none-any.whl (166 kB)\n","Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.1.0->transformers==4.15.0)\n","  Using cached typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n","Collecting charset-normalizer<4,>=2 (from requests->transformers==4.15.0)\n","  Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n","Collecting idna<4,>=2.5 (from requests->transformers==4.15.0)\n","  Using cached idna-3.4-py3-none-any.whl (61 kB)\n","Collecting urllib3<3,>=1.21.1 (from requests->transformers==4.15.0)\n","  Using cached urllib3-2.1.0-py3-none-any.whl (104 kB)\n","Collecting certifi>=2017.4.17 (from requests->transformers==4.15.0)\n","  Using cached certifi-2023.11.17-py3-none-any.whl (162 kB)\n","Collecting click (from sacremoses->transformers==4.15.0)\n","  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n","Collecting joblib (from sacremoses->transformers==4.15.0)\n","  Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n","Building wheels for collected packages: tokenizers\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n","\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n","\u001b[0mFailed to build tokenizers\n","\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","metadata":{"id":"jRXvtgWrrdk5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700797222370,"user_tz":-540,"elapsed":28725,"user":{"displayName":"김소현","userId":"12131249549978937836"}},"outputId":"85cd9e21-97e3-4889-cbf4-22db965fbf7d"},"source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","from transformers import *\n","import json\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import os\n","import sentencepiece as spm\n","from sklearn.model_selection import train_test_split\n","import tensorflow_addons as tfa\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from sklearn.metrics import confusion_matrix\n","import logging"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation_flax_utils.py:24: FutureWarning: Importing `FlaxGenerationMixin` from `src/transformers/generation_flax_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import FlaxGenerationMixin` instead.\n","  warnings.warn(\n","No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n","/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n","\n","TensorFlow Addons (TFA) has ended development and introduction of new features.\n","TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n","Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n","\n","For more information see: https://github.com/tensorflow/addons/issues/2807 \n","\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","metadata":{"id":"tTncidebqMqT"},"source":["구글 드라이브와 Colab을 연동합니다."]},{"cell_type":"code","metadata":{"id":"1JSqucXTB-jX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700797225474,"user_tz":-540,"elapsed":3119,"user":{"displayName":"김소현","userId":"12131249549978937836"}},"outputId":"3190a215-f694-4d38-e410-278e39e8e57d"},"source":["import os\n","from google.colab import drive\n","drive.mount('/content/Mydrive/')"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/Mydrive/; to attempt to forcibly remount, call drive.mount(\"/content/Mydrive/\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import os\n","import sys\n","\n","print(os.getcwd())"],"metadata":{"id":"qsqDZ2X1ojYW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700797225474,"user_tz":-540,"elapsed":37,"user":{"displayName":"김소현","userId":"12131249549978937836"}},"outputId":"e4cb21cd-39f1-45f4-bf83-fd0895f1fcdb"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["%cd korean-hate-speech-detection\n","\n","!pwd\n","!ls"],"metadata":{"id":"gEGCJgMhJ-Jx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700797225474,"user_tz":-540,"elapsed":25,"user":{"displayName":"김소현","userId":"12131249549978937836"}},"outputId":"6f335c62-fd20-46ef-c665-ac5c62c0e90a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: 'korean-hate-speech-detection'\n","/content\n","/content\n","Mydrive  sample_data\n"]}]},{"cell_type":"markdown","metadata":{"id":"5JFTyGGxwIwI"},"source":["딥러닝 훈련에 사용 할 train 데이터와 test 데이터를 pandas dataframe 형식으로 불러옵니다."]},{"cell_type":"code","metadata":{"id":"3XeWU_2OsRJ3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700797225474,"user_tz":-540,"elapsed":19,"user":{"displayName":"김소현","userId":"12131249549978937836"}},"outputId":"7056e2c0-8c83-44fc-8e5e-be1a3ee35b45"},"source":["data = pd.read_csv('/content/Mydrive/MyDrive/GoogleML/korean-hate-speech-detection/train.hate.csv')\n","\n","data_x = data['comments']\n","data_y = data['label']\n","\n","print(data_x.shape)\n","print(data_y.shape)\n","\n","LABEL_DIC = {\n","    'none': 0,\n","    'offensive': 1,\n","    'hate': 2,\n","}\n","\n","data_y = data['label'].map(lambda x: LABEL_DIC[x])\n","\n","print(data_x.shape)\n","print(data_y.shape)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["(7896,)\n","(7896,)\n","(7896,)\n","(7896,)\n"]}]},{"cell_type":"code","source":["data_y"],"metadata":{"id":"HYzCK7LDOyJk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700797225474,"user_tz":-540,"elapsed":16,"user":{"displayName":"김소현","userId":"12131249549978937836"}},"outputId":"e8579c40-7e7c-4d4b-9335-9b53ab9f151c"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0       2\n","1       0\n","2       2\n","3       0\n","4       2\n","       ..\n","7891    0\n","7892    0\n","7893    0\n","7894    0\n","7895    0\n","Name: label, Length: 7896, dtype: int64"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(data_x, data_y,\n","                 stratify = data_y,\n","                 test_size = 0.3,\n","                 random_state = 15)\n","\n","train = pd.concat([X_train, y_train], axis=1).reset_index()\n","test = pd.concat([X_test, y_test], axis=1).reset_index()\n","\n","print(f'Train: {train.shape}')\n","print(f'Test: {test.shape}')"],"metadata":{"id":"juwXygDGOT2a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700797225474,"user_tz":-540,"elapsed":12,"user":{"displayName":"김소현","userId":"12131249549978937836"}},"outputId":"433aa864-7e97-433a-d667-5d736ff01283"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Train: (5527, 3)\n","Test: (2369, 3)\n"]}]},{"cell_type":"code","metadata":{"id":"FaZFY6PlOr-U","colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"status":"ok","timestamp":1700797225475,"user_tz":-540,"elapsed":10,"user":{"displayName":"김소현","userId":"12131249549978937836"}},"outputId":"06034ed8-5556-4952-fb8c-d2e8127c1a3f"},"source":["test"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      index                                           comments  label\n","0      6065  이혼을 하더라도 박한별 직업은 연예인이고 먹고살아야하는데 방송에도 나오지말라하는건 ...      0\n","1      1724                               노래방은 여윽시 도우미를 불러야 제맛      1\n","2      1310                꼴랑 이백 ,삼백?너무 한거 아녀?이억,삼억 해도 모자랄판인데!      0\n","3      6940                                      지효 남친임 넘보지 마셈      0\n","4      1432            난 남자가 바람피워 이혼 그냥 생김세가 바람필거같어 아니면 참고 살던가      2\n","...     ...                                                ...    ...\n","2364    449                                   걍 폐지해라요즘 무한도전 노잼      1\n","2365   6319                               장윤정 결혼잘했지..가수나부랭이주제에      1\n","2366   4869                                     연기 최고였습니다...ㅠㅠ      0\n","2367   6175            잉? 오나라 언니랑 어떻게부부를?? 그러기엔 ....엄청어리지않아요??      1\n","2368   2770           민현이랑 라원이 잘 봤어요 진짜 갓기 둘이서 예방접종이라니ㅜㅠㅠㅠㅠㅠㅠㅠ      0\n","\n","[2369 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-17d56577-6c76-4cf0-a86a-89f0ee711247\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>comments</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6065</td>\n","      <td>이혼을 하더라도 박한별 직업은 연예인이고 먹고살아야하는데 방송에도 나오지말라하는건 ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1724</td>\n","      <td>노래방은 여윽시 도우미를 불러야 제맛</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1310</td>\n","      <td>꼴랑 이백 ,삼백?너무 한거 아녀?이억,삼억 해도 모자랄판인데!</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6940</td>\n","      <td>지효 남친임 넘보지 마셈</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1432</td>\n","      <td>난 남자가 바람피워 이혼 그냥 생김세가 바람필거같어 아니면 참고 살던가</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2364</th>\n","      <td>449</td>\n","      <td>걍 폐지해라요즘 무한도전 노잼</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2365</th>\n","      <td>6319</td>\n","      <td>장윤정 결혼잘했지..가수나부랭이주제에</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2366</th>\n","      <td>4869</td>\n","      <td>연기 최고였습니다...ㅠㅠ</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2367</th>\n","      <td>6175</td>\n","      <td>잉? 오나라 언니랑 어떻게부부를?? 그러기엔 ....엄청어리지않아요??</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2368</th>\n","      <td>2770</td>\n","      <td>민현이랑 라원이 잘 봤어요 진짜 갓기 둘이서 예방접종이라니ㅜㅠㅠㅠㅠㅠㅠㅠ</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2369 rows × 3 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-17d56577-6c76-4cf0-a86a-89f0ee711247')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-17d56577-6c76-4cf0-a86a-89f0ee711247 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-17d56577-6c76-4cf0-a86a-89f0ee711247');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-47b2d23a-ef4b-4dc9-b71b-e7fd879913ac\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-47b2d23a-ef4b-4dc9-b71b-e7fd879913ac')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-47b2d23a-ef4b-4dc9-b71b-e7fd879913ac button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"28aO4lGrwMjD"},"source":["## 버트 인풋 만들기"]},{"cell_type":"markdown","metadata":{"id":"_4JGys9DrU4_"},"source":["한글 데이터를 분석하려면, 100개가 넘는 언어에 대해 훈련된 버트를 사용해야 합니다.  \n","이번에는 한국어 데이터로 훈련되었고, SKT에서 만든 KoBERT를 사용하도록 하겠습니다.  \n","모델을 로드하기에 앞서, 토크나이저를 불러오도록 하겠습니다.  \n","huggingface에서는 아주 쉽게 토크나이저를 불러올 수 있습니다.  \n","https://github.com/monologg/KoBERT-NER 에서 kobert를 tokenize 할 수 있는 코드를 가져왔습니다."]},{"cell_type":"code","metadata":{"id":"ro5nKW78aktM","executionInfo":{"status":"ok","timestamp":1700797225475,"user_tz":-540,"elapsed":8,"user":{"displayName":"김소현","userId":"12131249549978937836"}}},"source":["import logging\n","import os\n","import unicodedata\n","from shutil import copyfile\n","\n","from transformers import PreTrainedTokenizer\n","\n","\n","logger = logging.getLogger(__name__)\n","\n","VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer_78b3253a26.model\",\n","                     \"vocab_txt\": \"vocab.txt\"}\n","\n","PRETRAINED_VOCAB_FILES_MAP = {\n","    \"vocab_file\": {\n","        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n","        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n","        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\"\n","    },\n","    \"vocab_txt\": {\n","        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n","        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n","        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\"\n","    }\n","}\n","\n","PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n","    \"monologg/kobert\": 512,\n","    \"monologg/kobert-lm\": 512,\n","    \"monologg/distilkobert\": 512\n","}\n","\n","PRETRAINED_INIT_CONFIGURATION = {\n","    \"monologg/kobert\": {\"do_lower_case\": False},\n","    \"monologg/kobert-lm\": {\"do_lower_case\": False},\n","    \"monologg/distilkobert\": {\"do_lower_case\": False}\n","}\n","\n","SPIECE_UNDERLINE = u'▁'\n","\n","\n","class KoBertTokenizer(PreTrainedTokenizer):\n","    \"\"\"\n","        SentencePiece based tokenizer. Peculiarities:\n","            - requires `SentencePiece <https://github.com/google/sentencepiece>`_\n","    \"\"\"\n","    vocab_files_names = VOCAB_FILES_NAMES\n","    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n","    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n","    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n","\n","    def __init__(\n","            self,\n","            vocab_file,\n","            vocab_txt,\n","            do_lower_case=False,\n","            remove_space=True,\n","            keep_accents=False,\n","            unk_token=\"[UNK]\",\n","            sep_token=\"[SEP]\",\n","            pad_token=\"[PAD]\",\n","            cls_token=\"[CLS]\",\n","            mask_token=\"[MASK]\",\n","            **kwargs):\n","        super().__init__(\n","            unk_token=unk_token,\n","            sep_token=sep_token,\n","            pad_token=pad_token,\n","            cls_token=cls_token,\n","            mask_token=mask_token,\n","            **kwargs\n","        )\n","\n","        # Build vocab\n","        self.token2idx = dict()\n","        self.idx2token = []\n","        with open(vocab_txt, 'r', encoding='utf-8') as f:\n","            for idx, token in enumerate(f):\n","                token = token.strip()\n","                self.token2idx[token] = idx\n","                self.idx2token.append(token)\n","\n","        try:\n","            import sentencepiece as spm\n","        except ImportError:\n","            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n","                           \"pip install sentencepiece\")\n","\n","        self.do_lower_case = do_lower_case\n","        self.remove_space = remove_space\n","        self.keep_accents = keep_accents\n","        self.vocab_file = vocab_file\n","        self.vocab_txt = vocab_txt\n","\n","        self.sp_model = spm.SentencePieceProcessor()\n","        self.sp_model.Load(vocab_file)\n","\n","    @property\n","    def vocab_size(self):\n","        return len(self.idx2token)\n","\n","    def get_vocab(self):\n","        return dict(self.token2idx, **self.added_tokens_encoder)\n","\n","    def __getstate__(self):\n","        state = self.__dict__.copy()\n","        state[\"sp_model\"] = None\n","        return state\n","\n","    def __setstate__(self, d):\n","        self.__dict__ = d\n","        try:\n","            import sentencepiece as spm\n","        except ImportError:\n","            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n","                           \"pip install sentencepiece\")\n","        self.sp_model = spm.SentencePieceProcessor()\n","        self.sp_model.Load(self.vocab_file)\n","\n","    def preprocess_text(self, inputs):\n","        if self.remove_space:\n","            outputs = \" \".join(inputs.strip().split())\n","        else:\n","            outputs = inputs\n","        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n","\n","        if not self.keep_accents:\n","            outputs = unicodedata.normalize('NFKD', outputs)\n","            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n","        if self.do_lower_case:\n","            outputs = outputs.lower()\n","\n","        return outputs\n","\n","    def _tokenize(self, text, return_unicode=True, sample=False):\n","        \"\"\" Tokenize a string. \"\"\"\n","        text = self.preprocess_text(text)\n","\n","        if not sample:\n","            pieces = self.sp_model.EncodeAsPieces(text)\n","        else:\n","            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n","        new_pieces = []\n","        for piece in pieces:\n","            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n","                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n","                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n","                    if len(cur_pieces[0]) == 1:\n","                        cur_pieces = cur_pieces[1:]\n","                    else:\n","                        cur_pieces[0] = cur_pieces[0][1:]\n","                cur_pieces.append(piece[-1])\n","                new_pieces.extend(cur_pieces)\n","            else:\n","                new_pieces.append(piece)\n","\n","        return new_pieces\n","\n","    def _convert_token_to_id(self, token):\n","        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n","        return self.token2idx.get(token, self.token2idx[self.unk_token])\n","\n","    def _convert_id_to_token(self, index, return_unicode=True):\n","        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n","        return self.idx2token[index]\n","\n","    def convert_tokens_to_string(self, tokens):\n","        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n","        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n","        return out_string\n","\n","    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n","        \"\"\"\n","        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n","        by concatenating and adding special tokens.\n","        A KoBERT sequence has the following format:\n","            single sequence: [CLS] X [SEP]\n","            pair of sequences: [CLS] A [SEP] B [SEP]\n","        \"\"\"\n","        if token_ids_1 is None:\n","            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n","        cls = [self.cls_token_id]\n","        sep = [self.sep_token_id]\n","        return cls + token_ids_0 + sep + token_ids_1 + sep\n","\n","    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n","        \"\"\"\n","        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n","        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n","        Args:\n","            token_ids_0: list of ids (must not contain special tokens)\n","            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n","                for sequence pairs\n","            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n","                special tokens for the model\n","        Returns:\n","            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n","        \"\"\"\n","\n","        if already_has_special_tokens:\n","            if token_ids_1 is not None:\n","                raise ValueError(\n","                    \"You should not supply a second sequence if the provided sequence of \"\n","                    \"ids is already formated with special tokens for the model.\"\n","                )\n","            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n","\n","        if token_ids_1 is not None:\n","            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n","        return [1] + ([0] * len(token_ids_0)) + [1]\n","\n","    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n","        \"\"\"\n","        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n","        A KoBERT sequence pair mask has the following format:\n","        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n","        | first sequence    | second sequence\n","        if token_ids_1 is None, only returns the first portion of the mask (0's).\n","        \"\"\"\n","        sep = [self.sep_token_id]\n","        cls = [self.cls_token_id]\n","        if token_ids_1 is None:\n","            return len(cls + token_ids_0 + sep) * [0]\n","        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n","\n","    def save_vocabulary(self, save_directory):\n","        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\n","            to a directory.\n","        \"\"\"\n","        if not os.path.isdir(save_directory):\n","            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n","            return\n","\n","        # 1. Save sentencepiece model\n","        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n","\n","        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\n","            copyfile(self.vocab_file, out_vocab_model)\n","\n","        # 2. Save vocab.txt\n","        index = 0\n","        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\n","        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\n","            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\n","                if index != token_index:\n","                    logger.warning(\n","                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n","                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\n","                    )\n","                    index = token_index\n","                writer.write(token + \"\\n\")\n","                index += 1\n","\n","        return out_vocab_model, out_vocab_txt"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rWK4I6oC44Or"},"source":["kobert 토크나이즈를 임포트합니다."]},{"cell_type":"code","metadata":{"id":"g4Xgl9RtsGul","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700797612240,"user_tz":-540,"elapsed":634,"user":{"displayName":"김소현","userId":"12131249549978937836"}},"outputId":"1d9e0cc3-9dfe-494e-e0e8-c345e4225b0e"},"source":["tokenizer = BertTokenizer.from_pretrained('monologg/kobert', bos_token='</s>', eos_token='</s>', pad_token='<pad>', mask_token='<mask>')"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--monologg--kobert/snapshots/30941062c0f3dde73b246468f449f2448c7694bc/vocab.txt\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--monologg--kobert/snapshots/30941062c0f3dde73b246468f449f2448c7694bc/tokenizer_config.json\n","loading file tokenizer.json from cache at None\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--monologg--kobert/snapshots/30941062c0f3dde73b246468f449f2448c7694bc/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"monologg/kobert\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.35.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 8002\n","}\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"ii2W5UbyuN2b"},"source":["버트를 사용하기에 앞서 가장 기초에 속하는 tokenizer 사용 방법에 대해서 잠시 배워보도록 하겠습니다.  \n","tokenizer.encode => 문장을 버트 모델의 인풋 토큰값으로 바꿔줌  \n","tokenizer.tokenize => 문장을 토큰화"]},{"cell_type":"code","metadata":{"id":"LFFokLO0sj_L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700797629477,"user_tz":-540,"elapsed":3676,"user":{"displayName":"김소현","userId":"12131249549978937836"}},"outputId":"e803cb7b-a19e-4c56-a3c3-c0074fb3c258"},"source":["def convert_data(data_df):\n","    global tokenizer\n","\n","    SEQ_LEN = 128 #SEQ_LEN : 버트에 들어갈 인풋의 길이\n","\n","    tokens, masks, segments, targets = [], [], [], []\n","\n","    for i in tqdm(range(len(data_df))):\n","        # token : 문장을 토큰화함\n","        token = tokenizer.encode(data_df[DATA_COLUMN][i], truncation=True, padding='max_length', max_length=SEQ_LEN)\n","\n","        # 마스크는 토큰화한 문장에서 패딩이 아닌 부분은 1, 패딩인 부분은 0으로 통일\n","        num_zeros = token.count(0)\n","        mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros\n","\n","        # 문장의 전후관계를 구분해주는 세그먼트는 문장이 1개밖에 없으므로 모두 0\n","        segment = [0]*SEQ_LEN\n","\n","        # 버트 인풋으로 들어가는 token, mask, segment를 tokens, segments에 각각 저장\n","        tokens.append(token)\n","        masks.append(mask)\n","        segments.append(segment)\n","\n","        # 정답(1 = 혐오표현,0 = 비혐오표현)을 targets 변수에 저장해 줌\n","        targets.append(data_df[LABEL_COLUMN][i])\n","\n","    # tokens, masks, segments, 정답 변수 targets를 numpy array로 지정\n","    tokens = np.array(tokens)\n","    masks = np.array(masks)\n","    segments = np.array(segments)\n","    targets = np.array(targets)\n","\n","    return [tokens, masks, segments], targets\n","\n","# 위에 정의한 convert_data 함수를 불러오는 함수를 정의\n","def load_data(pandas_dataframe):\n","    data_df = pandas_dataframe\n","    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n","    data_df[LABEL_COLUMN] = data_df[LABEL_COLUMN].astype(int)\n","    data_x, data_y = convert_data(data_df)\n","    return data_x, data_y\n","\n","SEQ_LEN = 128\n","BATCH_SIZE = 32\n","# 긍부정 문장을 포함하고 있는 칼럼\n","DATA_COLUMN = \"comments\"\n","# 긍정인지 부정인지를 (1 = 혐오표현, 0 = 비혐오표현) 포함하고 있는 칼럼\n","LABEL_COLUMN = \"label\"\n","\n","# train 데이터를 버트 인풋에 맞게 변환\n","train_x, train_y = load_data(train)"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 5527/5527 [00:03<00:00, 1836.39it/s]\n"]}]},{"cell_type":"code","metadata":{"id":"9ocb17LekZVn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700797637009,"user_tz":-540,"elapsed":1172,"user":{"displayName":"김소현","userId":"12131249549978937836"}},"outputId":"5ba6a8d7-5fb7-451d-efe1-31c8ff1cc1e3"},"source":["# 훈련 성능을 검증한 test 데이터를 버트 인풋에 맞게 변환\n","test_x, test_y = load_data(test)"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 2369/2369 [00:00<00:00, 3113.02it/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"45UY1wudwa3Q"},"source":["## 버트를 활용한 감성분석 모델 만들기"]},{"cell_type":"code","metadata":{"id":"W85BlYfo2Wge","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700797649998,"user_tz":-540,"elapsed":9879,"user":{"displayName":"김소현","userId":"12131249549978937836"}},"outputId":"26e6713d-b2b7-473a-a802-89c3b6cd88d5"},"source":["model = TFBertModel.from_pretrained(\"monologg/kobert\", from_pt=True)\n","# 토큰 인풋, 마스크 인풋, 세그먼트 인풋 정의\n","token_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_word_ids')\n","mask_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_masks')\n","segment_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_segment')\n","\n","# 인풋이 [토큰, 마스크, 세그먼트]인 모델 정의\n","bert_outputs = model([token_inputs, mask_inputs, segment_inputs])\n","bert_outputs = bert_outputs[1]\n","bert_outputs"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--monologg--kobert/snapshots/30941062c0f3dde73b246468f449f2448c7694bc/config.json\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.35.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 8002\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--monologg--kobert/snapshots/30941062c0f3dde73b246468f449f2448c7694bc/pytorch_model.bin\n","Loading PyTorch weights from /root/.cache/huggingface/hub/models--monologg--kobert/snapshots/30941062c0f3dde73b246468f449f2448c7694bc/pytorch_model.bin\n","PyTorch checkpoint contains 92,186,880 parameters\n","Loaded 92,186,880 parameters in the TF 2.0 model.\n","All PyTorch model weights were used when initializing TFBertModel.\n","\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"execute_result","data":{"text/plain":["<KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_bert_model_1')>"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"CXKy-Jsg3eKA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700797649998,"user_tz":-540,"elapsed":15,"user":{"displayName":"김소현","userId":"12131249549978937836"}},"outputId":"6c886a56-bcab-4c12-a76b-f2b2167905b5"},"source":["sentiment_drop = tf.keras.layers.Dropout(0.5)(bert_outputs)\n","sentiment_first = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))(sentiment_drop)\n","sentiment_model = tf.keras.Model([token_inputs, mask_inputs, segment_inputs], sentiment_first)\n","\n","opt = tfa.optimizers.RectifiedAdam(lr=5.0e-5, total_steps = 2344*2, warmup_proportion=0.1, min_lr=1e-5, epsilon=1e-08, clipnorm=1.0)\n","sentiment_model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(), metrics = ['accuracy'])\n","sentiment_model.summary()"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_word_ids (InputLayer  [(None, 127)]                0         []                            \n"," )                                                                                                \n","                                                                                                  \n"," input_masks (InputLayer)    [(None, 127)]                0         []                            \n","                                                                                                  \n"," input_segment (InputLayer)  [(None, 127)]                0         []                            \n","                                                                                                  \n"," tf_bert_model_1 (TFBertMod  TFBaseModelOutputWithPooli   9218688   ['input_word_ids[0][0]',      \n"," el)                         ngAndCrossAttentions(last_   0          'input_masks[0][0]',         \n","                             hidden_state=(None, 127, 7              'input_segment[0][0]']       \n","                             68),                                                                 \n","                              pooler_output=(None, 768)                                           \n","                             , past_key_values=None, hi                                           \n","                             dden_states=None, attentio                                           \n","                             ns=None, cross_attentions=                                           \n","                             None)                                                                \n","                                                                                                  \n"," dropout_75 (Dropout)        (None, 768)                  0         ['tf_bert_model_1[0][1]']     \n","                                                                                                  \n"," dense_1 (Dense)             (None, 1)                    769       ['dropout_75[0][0]']          \n","                                                                                                  \n","==================================================================================================\n","Total params: 92187649 (351.67 MB)\n","Trainable params: 92187649 (351.67 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"yI0_9hOAwp1-"},"source":["## 훈련 및 성능 검증"]},{"cell_type":"code","metadata":{"id":"EXuNkLlUbHN3","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1700797743442,"user_tz":-540,"elapsed":93452,"user":{"displayName":"김소현","userId":"12131249549978937836"}},"outputId":"a289d8a7-4269-4f47-88e1-49ced20ebdc5"},"source":["es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5, restore_best_weights = True)\n","sentiment_model.fit(train_x, train_y, epochs=10, shuffle=True, batch_size=32, callbacks=[es], validation_data=(test_x, test_y))"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n"]},{"output_type":"error","ename":"InvalidArgumentError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-2cb328eee923>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msentiment_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     ]\n\u001b[0;32m---> 60\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     61\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node model_1/tf_bert_model_1/bert/embeddings/assert_less/Assert/Assert defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-25-2cb328eee923>\", line 2, in <cell line: 2>\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1783, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1377, in train_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1360, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1349, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1126, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 589, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 515, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 672, in _run_internal_graph\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 589, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\", line 1061, in run_call_with_unpacked_inputs\n\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_tf_bert.py\", line 1088, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\", line 1061, in run_call_with_unpacked_inputs\n\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_tf_bert.py\", line 780, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_tf_bert.py\", line 201, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_tf_bert.py\", line 202, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/tf_utils.py\", line 161, in check_embeddings_within_bounds\n\nassertion failed: [The maximum value of input_ids (Tensor(\\\"model_1/tf_bert_model_1/bert/embeddings/Max:0\\\", shape=(), dtype=int32)) must be smaller than the embedding layer\\'s input dimension (8002). The likely cause is some problem at tokenization time.] [Condition x < y did not hold element-wise:] [x (model_1/Cast:0) = ] [[2 0 0...]...] [y (model_1/tf_bert_model_1/bert/embeddings/Cast/x:0) = ] [8002]\n\t [[{{node model_1/tf_bert_model_1/bert/embeddings/assert_less/Assert/Assert}}]] [Op:__inference_train_function_82619]"]}]},{"cell_type":"code","source":["sentiment_model"],"metadata":{"id":"dKLt36N4mkRr","executionInfo":{"status":"aborted","timestamp":1700797337848,"user_tz":-540,"elapsed":16,"user":{"displayName":"김소현","userId":"12131249549978937836"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentiment_model.summary()"],"metadata":{"id":"zhNuHiTeQ280","executionInfo":{"status":"aborted","timestamp":1700797337849,"user_tz":-540,"elapsed":17,"user":{"displayName":"김소현","userId":"12131249549978937836"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 5))\n","\n","# 오차\n","y_vloss = sentiment_model.history.history['val_loss']\n","\n","# 학습셋 오차\n","y_loss = sentiment_model.history.history['loss']\n","\n","# 그래프로 표현\n","x_len = np.arange(len(y_loss))\n","ax1.plot(x_len, y_vloss, marker = '.', c=\"red\", label='Testset_loss')\n","ax1.plot(x_len, y_loss, marker = '.', c='blue', label = 'Trainset_loss')\n","\n","# 그래프에 그리드를 주고 레이블을 표시\n","ax1.legend(loc='upper right')\n","ax1.grid()\n","ax1.set(xlabel='epoch', ylabel='loss')\n","\n","\n","# 정확도\n","y_vaccuracy = sentiment_model.history.history['val_accuracy']\n","\n","# 학습셋\n","y_accuracy = sentiment_model.history.history['accuracy']\n","\n","# 그래프로 표현\n","x_len = np.arange(len(y_accuracy))\n","ax2.plot(x_len, y_vaccuracy, marker = '.', c=\"red\", label='Testset_accuracy')\n","ax2.plot(x_len, y_accuracy, marker = '.', c='blue', label = 'Trainset_accuracy')\n","\n","# 그래프에 그리드를 주고 레이블을 표시\n","ax2.legend(loc='lower right')\n","ax2.grid()\n","\n","ax2.set(xlabel='epoch', ylabel='accuracy')\n","\n","# draw gridlines\n","ax2.grid(True)\n","plt.show()"],"metadata":{"id":"Om9lOYmjr021","executionInfo":{"status":"aborted","timestamp":1700797337849,"user_tz":-540,"elapsed":16,"user":{"displayName":"김소현","userId":"12131249549978937836"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J1Gv3m_34nIX"},"source":["훈련 모델의 예측 성능을 F1 SCORE로 체크하기 위한 작업"]},{"cell_type":"code","metadata":{"id":"Zjv6RMR1jYIe","executionInfo":{"status":"aborted","timestamp":1700797337849,"user_tz":-540,"elapsed":16,"user":{"displayName":"김소현","userId":"12131249549978937836"}}},"source":["def predict_convert_data(data_df):\n","    global tokenizer\n","    tokens, masks, segments = [], [], []\n","\n","    for i in tqdm(range(len(data_df))):\n","\n","        token = tokenizer.encode(data_df[DATA_COLUMN][i], max_length=SEQ_LEN, truncation=True, padding='max_length')\n","        num_zeros = token.count(0)\n","        mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros\n","        segment = [0]*SEQ_LEN\n","\n","        tokens.append(token)\n","        segments.append(segment)\n","        masks.append(mask)\n","\n","    tokens = np.array(tokens)\n","    masks = np.array(masks)\n","    segments = np.array(segments)\n","    return [tokens, masks, segments]\n","\n","# 위에 정의한 convert_data 함수를 불러오는 함수를 정의\n","def predict_load_data(pandas_dataframe):\n","    data_df = pandas_dataframe\n","    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n","    data_x = predict_convert_data(data_df)\n","    return data_x\n","\n","# 각 지표 구하는 함수 구현\n","def model_evaluation(label, predict):\n","    cf_matrix = confusion_matrix(label, predict)\n","    Accuracy = (cf_matrix[0][0] + cf_matrix[1][1]) / sum(sum(cf_matrix))\n","    Precision = cf_matrix[1][1] / (cf_matrix[1][1] + cf_matrix[0][1])\n","    Recall = cf_matrix[1][1] / (cf_matrix[1][1] + cf_matrix[1][0])\n","    Specificity = cf_matrix[0][0] / (cf_matrix[0][0] + cf_matrix[0][1])\n","    F1_Score = (2 * Recall * Precision) / (Recall + Precision)\n","\n","    print(f'Accuracy: {Accuracy}')\n","    print(f'Precision: {Precision}')\n","    print(f'Recall: {Recall}')\n","    print(f'Specificity: {Specificity}')\n","    print(f'F1_Score: {F1_Score}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1nyqBoQD4tz1"},"source":["test 데이터 예측하기"]},{"cell_type":"code","metadata":{"id":"c5e3LiY1t3MO","executionInfo":{"status":"aborted","timestamp":1700797337849,"user_tz":-540,"elapsed":16,"user":{"displayName":"김소현","userId":"12131249549978937836"}}},"source":["test2 = pd.read_csv('/content/Mydrive/MyDrive/1129/tsv_Counter speech.tsv', sep = '\\t')\n","test_set = predict_load_data(test2)\n","proba = sentiment_model.predict(test_set)\n","\n","test2['probability'] = proba\n","test2['pred'] = np.round(proba, 0)\n","test2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H5I01l34VLu3","executionInfo":{"status":"aborted","timestamp":1700797337849,"user_tz":-540,"elapsed":15,"user":{"displayName":"김소현","userId":"12131249549978937836"}}},"source":["test2.to_csv('/content/Mydrive/MyDrive/1129/prediction_result_epoch10.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tzlfYX6xlFEk","executionInfo":{"status":"aborted","timestamp":1700797337849,"user_tz":-540,"elapsed":15,"user":{"displayName":"김소현","userId":"12131249549978937836"}}},"source":["# 사고관련 --> 0을 출력\n","y_true = test2['hate']\n","\n","model_evaluation(y_true, np.round(proba, 0))\n","confusion_matrix(y_true, np.round(proba, 0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import classification_report, confusion_matrix\n","from itertools import product\n","\n","def plot_confusion_matrix(cm, classes,\n","                        normalize=False,\n","                        title='Confusion matrix',\n","                        cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    if normalize:\n","        cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], 2)\n","\n","    thresh = cm.max() / 2.\n","    for i, j in product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, cm[i, j],\n","            horizontalalignment=\"center\",\n","            color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","\n","\n","validation_predictions = [np.argmax(i) for i in sentiment_model.predict(test_set)] #predictions\n","validation_labels = test2.label.values.tolist() #ground truth labels\n","\n","cm_plot_labels = ['neutral', 'contradiction']\n","cm = confusion_matrix(y_true=validation_labels, y_pred=validation_predictions)\n","plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix Without Normalization')\n","# plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix With Normalization', normalize=True)\n","\n","target_class = ['neutral' if label==0 else 'contradiction' for label in validation_labels]\n","prediction_class = ['neutral' if label==0 else 'contradiction' for label in validation_predictions]\n","print('\\nClassification Report')\n","print(classification_report(y_true=target_class, y_pred=prediction_class))"],"metadata":{"id":"nc6JQAlzU4PW","executionInfo":{"status":"aborted","timestamp":1700797337849,"user_tz":-540,"elapsed":15,"user":{"displayName":"김소현","userId":"12131249549978937836"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_predictions_baseline = model.predict(train_features, batch_size=BATCH_SIZE)\n","test_predictions_baseline = model.predict(test_features, batch_size=BATCH_SIZE)"],"metadata":{"id":"RssSfsFCW2rO","executionInfo":{"status":"aborted","timestamp":1700797337850,"user_tz":-540,"elapsed":14,"user":{"displayName":"김소현","userId":"12131249549978937836"}}},"execution_count":null,"outputs":[]}]}